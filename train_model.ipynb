{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation\n",
    "\n",
    "First we need to generate the dataset used for the training. We do this using the AFM simulation code Probe Particle Model from Prokop Hapala et al. \n",
    "\n",
    "The Probe Particle Model does not have a python package so we need to manually download the repository from github. We will use a specific branch and commit in order to make sure that the results are replicatable.\n",
    "```console\n",
    "git clone https://github.com/ProkopHapala/ProbeParticleModel.git\n",
    "cd ProbeParticleModel\n",
    "git checkout 7f162d0cfe034b352b2280f31f6a5f8e19d83693\n",
    "```\n",
    "Take note of the directory where you saved the repository and set the path to the variable `PPMdir` below.\n",
    "\n",
    "In order for ProbeParticleModel to run correctly we need to tell pyopencl where to find nvidia drivers. On Linux within a conda environment, the following should work:\n",
    "```console\n",
    "cp /etc/OpenCL/vendors/nvidia.icd $CONDA_PREFIX'/etc/OpenCL/vendors/'\n",
    "```\n",
    "\n",
    "We also need the molecule geometries that the AFM simulation takes as an input. Download these from here: https://www.dropbox.com/s/g6ngxz2qsju94db/Molecules_xyz3.tar?dl=0. Extract the archive and set the path of the directory to the variable `Moldir` below.\n",
    "\n",
    "Set the path to the directory where you want to save the dataset to the variable `save_dir`. __Note: the full dataset takes about ~250GB of space.__\n",
    "\n",
    "Choose the dataset that you want to generate by setting the variable `dataset` to either `'light'` or `'heavy'`. The words light and heavy do not refer to the sizes of the datasets but rather the atomic elements which they contain. The light dataset contains only the elements H, C, N, O, and F, and the heavy dataset additionally contains Si, P, S, Cl, and Br."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMdir = './ProbeParticleModel/'    # Path to the ProbeParticleModel repository\n",
    "Moldir = './Molecules3/'            # Path to the molecule geometry files\n",
    "dataset = 'light'                   # Or 'heavy'. Which dataset to generate\n",
    "data_dir = './Data_%s/' % dataset   # Directory where to save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set all the options for the data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator options\n",
    "gen_args = {\n",
    "    'PPMdir'            : PPMdir,       # Probe Particle Model repository directory\n",
    "    'preName'           : Moldir,       # Molecules are read from filename = preName + molecules[imol] + postName\n",
    "    'postName'          : '.xyz',\n",
    "    'Ymode'             : 'D-S-H',      # Generator descriptor output mode\n",
    "    'batch_size'        : 30,           # Batch size\n",
    "    'nBestRotations'    : 30,           # Number of rotations for each molecule\n",
    "    'distAbove'         : 5.0,          # Distance between the scan start height and highest atom in the scan\n",
    "    'distAboveDelta'    : 0.25,         # Random variation range(+-) for distAbove\n",
    "    'scan_dim'          : (128,128,20), # Scan size (voxels), final z-size = scan_dim[2]-df_weight_steps (df_weight_steps = 10 by default)\n",
    "    'iZPPs'             : [8],          # Probe particle(s) (8=O, 54=Xe)\n",
    "    'Qs'                : [-0.1],       # Probe particle charges\n",
    "    'tip_type'          : 'monopole',   # Tip electrostatics model (monopole, dipole, quadrupole)\n",
    "    'maxTilt0'          : 0.5,\n",
    "    'diskMode'          : 'sphere',\n",
    "    'dzmax'             : 1.2,\n",
    "    'dzmax_s'           : 1.2\n",
    "}\n",
    "\n",
    "# Molecules\n",
    "if dataset == 'light':\n",
    "    N_train        = 5000      # Number of training molecules from light molecule set\n",
    "    N_val          = 1500      # Number of validation molecules from light molecule set\n",
    "    N_test         = 2500      # Number of test molecules from light molecule set\n",
    "    N_train_h      = 0         # Number of training molecules from heavy molecule set\n",
    "    N_val_h        = 0         # Number of validation molecules from heavy molecule set\n",
    "    N_test_h       = 0         # Number of test molecules from heavy molecule set\n",
    "elif dataset == 'heavy':\n",
    "    N_train        = 3500\n",
    "    N_val          = 900\n",
    "    N_test         = 1200\n",
    "    N_train_h      = 2500\n",
    "    N_val_h        = 600\n",
    "    N_test_h       = 1200\n",
    "else:\n",
    "    raise ValueError('Invalid dataset')\n",
    "\n",
    "# Heavy molecules\n",
    "train_molecules = ['heavy/'+str(n) for n in range(N_train_h)]\n",
    "val_molecules = ['heavy/'+str(n) for n in range(N_train_h,N_train_h+N_val_h)]\n",
    "test_molecules = ['heavy/'+str(n) for n in range(N_train_h+N_val_h,N_train_h+N_val_h+N_test_h)]\n",
    "\n",
    "# Light molecules\n",
    "train_molecules += ['light/'+str(n) for n in range(N_train)]\n",
    "val_molecules += ['light/'+str(n) for n in range(N_train,N_train+N_val)]\n",
    "test_molecules += ['light/'+str(n) for n in range(N_train+N_val,N_train+N_val+N_test)]\n",
    "\n",
    "# Make sure save directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the dataset. This will take at least half an hour (GTX 1080 + NVMe SSD), probably more depending on your GPU and your storage device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import construct_generator\n",
    "\n",
    "# Generate dataset\n",
    "start_time = time.time()\n",
    "counter = 1\n",
    "total_len = len(train_molecules)+len(val_molecules)+len(test_molecules)\n",
    "for mode, molecules in zip(['train', 'val', 'test'], [train_molecules, val_molecules, test_molecules]):\n",
    "\n",
    "    # Make generator\n",
    "    gen = construct_generator(molecules=molecules, **gen_args)\n",
    "\n",
    "    # Make sure target directory exists\n",
    "    target_dir = data_dir+mode+'/'\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    # Generate batches\n",
    "    for i in range(len(gen)):\n",
    "        batch = gen.next()\n",
    "        np.savez(target_dir+'batch_%d.npz'%i, batch[0].astype(np.float32), batch[1].astype(np.float32))\n",
    "        eta = (time.time() - start_time)/counter * (total_len - counter)\n",
    "        print('Generated %s batch %d/%d - ETA: %ds' % (mode, i+1, len(gen), eta))\n",
    "        counter += 1\n",
    "\n",
    "print('Total time taken: %d' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Now that we have generated the dataset, it is time to train the model. First we set some options and define the model architecture. The `loss_weights` parameter controls how much weight the optimization algorithm puts on the losses from the different output descriptors. The set values are something we found through empirical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights      = [20.0, 0.2, 0.1]                              # Weights for balancing the loss\n",
    "epochs            = 50                                            # How many epochs to train\n",
    "pred_batches      = 3                                             # How many batches to do predictions on\n",
    "model_dir         = './model_%s/' % dataset                       # Directory where all output files are saved to\n",
    "pred_dir          = os.path.join(model_dir, 'predictions/')       # Where to save predictions\n",
    "checkpoint_dir    = os.path.join(model_dir, 'checkpoints/')       # Where to save model checkpoints\n",
    "log_path          = os.path.join(model_dir, 'training.log')       # Where to save loss history during training\n",
    "history_plot_path = os.path.join(model_dir, 'loss_history.png')   # Where to plot loss history during training\n",
    "descriptors       = ['Atomic_Disks', 'vdW_Spheres', 'Height_Map'] # Used for outputting information\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the model and compile it. We use the Adam optimizer and mean squared error. See the file `models.py` for the model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from models import create_model\n",
    "\n",
    "model = create_model(last_relu=[False, True, True], out_labels=descriptors)\n",
    "optimizer = optimizers.Adam(lr=0.001, decay=1e-5)\n",
    "model.compile(optimizer, 'mse', loss_weights=loss_weights)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup data loading. The dataset is way too big to fit in memory on most systems so we need to use data generator that loads the batches from disk. We need one data generator for the training, one for the validation, and one for testing. The `DataGenerator` class will automatically read the saved batches in the directories we specify. We also define some callbacks to save information during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import DataGenerator\n",
    "\n",
    "train_gen = DataGenerator(os.path.join(data_dir, 'train/'))\n",
    "val_gen   = DataGenerator(os.path.join(data_dir, 'val/'))\n",
    "test_gen  = DataGenerator(os.path.join(data_dir, 'test/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from data_utils import HistoryPlotter\n",
    "checkpointer = ModelCheckpoint(checkpoint_dir+'weights_{epoch:d}.h5', save_weights_only=True)\n",
    "logger = CSVLogger(log_path, append=True)\n",
    "plotter = HistoryPlotter(log_path, history_plot_path, descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training takes a long time, it might be necessary to resume the training later. The below code will load the latest checkpoint if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume previous epoch if exists\n",
    "init_epoch = 0\n",
    "model_file = None\n",
    "for i in range(1, epochs+1):\n",
    "    cp_file = os.path.join(checkpoint_dir, 'weights_%d.h5' % i)\n",
    "    if os.path.exists(cp_file):\n",
    "        init_epoch += 1\n",
    "        model_file = cp_file\n",
    "    else:\n",
    "        break\n",
    "if init_epoch > 0:\n",
    "    model.load_weights(model_file)\n",
    "    print('Model weights loaded from '+cp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "Now we fit the model. __Expect this to take a long time__, on the order of a full day. You can follow the loss curve during training in the file specified in `history_plot_path`. The resulting loss curves should look similar to the ones in Figure S10 for the corresponding dataset, in the supplementary information of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model.fit_generator(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=epochs,\n",
    "    initial_epoch=init_epoch,\n",
    "    callbacks=[checkpointer, logger, plotter]\n",
    ")\n",
    "\n",
    "# Save final weights\n",
    "model.save_weights(os.path.join(model_dir, 'model.h5'))\n",
    "\n",
    "# Show loss history\n",
    "plotter.plot(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evalutation\n",
    "Test the model on the test set. This should take roughly half as long as one epoch during training. The final losses should be roughly the same on the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate_generator(test_gen, verbose=1)\n",
    "print('Losses on training set: '+str(plotter.losses[-1]))\n",
    "print('Losses on validation set: '+str(plotter.val_losses[-1]))\n",
    "print('Losses on test set: '+str(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make some predictions on the samples in the test set. The printed files are saved to the directory specified in `pred_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data_utils import make_prediction_plots, make_input_plots, calculate_losses\n",
    "\n",
    "for i in range(pred_batches):\n",
    "    X, true = test_gen[i]\n",
    "    preds = model.predict_on_batch(X)\n",
    "    losses = calculate_losses(model, true, preds)\n",
    "    make_prediction_plots(preds, true, losses, descriptors, pred_dir, start_ind=gen_args['batch_size']*i)\n",
    "    make_input_plots(X, pred_dir, start_ind=gen_args['batch_size']*i, constant_range=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
